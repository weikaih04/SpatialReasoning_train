version: v2
description: Molmo 2 Data Judger Eval - PAT - ckpt 0012920
budget: ai2/oe-mm
tasks:
- name: eval-path-tracing
  replicas: 1
  image:
    docker: pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel
  envVars:
    - name: WEIKAI_UW_WANDB_KEY
      secret: WEIKAI_UW_WANDB_KEY
  command: ['bash', '-c']
  arguments:
  - |
    # ============================================================================
    # Configuration - MODIFY THIS SECTION FOR EACH NEW CHECKPOINT
    # ============================================================================
    # üëà CHANGE THIS to your checkpoint path (new organized structure)
    # Format: ckpt/<model>/<run_name>/<step>
    export MODEL_PATH="ckpt/pat/run_8gpu/0012920"

    # Run name for organizing outputs (derived from MODEL_PATH)
    export RUN_NAME=$(echo ${MODEL_PATH} | cut -d'/' -f3)  # e.g., run_8gpu
    export STEP_NAME=$(basename ${MODEL_PATH})  # e.g., 0012920

    # Multi-GPU Configuration
    # For 8 GPUs: Use NPROC=2 (each process uses 4 GPUs)
    # For 4 GPUs: Use NPROC=2 (each process uses 2 GPUs) - RECOMMENDED
    # For 2 GPUs: Use NPROC=1 (each process uses 2 GPUs)
    export NPROC=2  # üëà Number of parallel processes (NOT number of GPUs!)
    # ============================================================================

    export DATASET_NAME=AI2ThorPathTracing
    export MODEL_NAME="thinkmorph_pat"  # Use registered model name from config.py
    export FULL_MODEL_PATH="/weka/oe-training-default/jieyuz2/improve_segments/visual_cot/ThinkMorph_training/${MODEL_PATH}"

    # Set environment variables for dynamic model loading (used by config.py)
    export THINKMORPH_MODEL_PATH="${FULL_MODEL_PATH}"
    export THINKMORPH_SAVE_DIR="/weka/oe-training-default/jieyuz2/improve_segments/visual_cot/ThinkMorph_training/results/pat/${RUN_NAME}/${STEP_NAME}"

    echo "=========================================="
    echo "Evaluation Configuration"
    echo "=========================================="
    echo "Model Path: ${MODEL_PATH}"
    echo "Full Path: ${FULL_MODEL_PATH}"
    echo "Dataset: ${DATASET_NAME}"
    echo "Model Name: ${MODEL_NAME}"
    echo "Run Name: ${RUN_NAME}"
    echo "Step: ${STEP_NAME}"
    echo "Save Dir: ${THINKMORPH_SAVE_DIR}"
    echo "=========================================="
    echo ""
    
    # Setup environment
    export DEBIAN_FRONTEND=noninteractive
    export TZ=America/Los_Angeles
    
    echo "Installing system dependencies..."
    apt-get update && \
    apt-get install -y libstdc++6 libgl1-mesa-glx libglib2.0-0 g++ git
    
    echo "Activating conda environment..."
    source /weka/oe-training-default/jieyuz2/improve_segments/miniconda3/etc/profile.d/conda.sh
    conda activate thinkmorph_eval
    
    cd /weka/oe-training-default/jieyuz2/improve_segments/visual_cot/ThinkMorph_training
    
    # Prepare checkpoint (copy config files if missing)
    echo ""
    echo "Preparing checkpoint for evaluation..."
    bash scripts/prepare_checkpoint_for_eval.sh ${MODEL_PATH}
    
    if [ $? -ne 0 ]; then
        echo "‚ùå Checkpoint preparation failed!"
        exit 1
    fi
    
    # Navigate to evaluation directory
    cd VLMEvalKit_Thinkmorph
    
    # Run evaluation with torchrun for multi-GPU acceleration
    echo ""
    echo "=========================================="
    echo "Starting Evaluation with Multi-GPU"
    echo "=========================================="
    echo "Running full evaluation on ${DATASET_NAME} (409 samples)..."
    echo ""

    # Get number of available GPUs
    NUM_GPUS=$(nvidia-smi --list-gpus | wc -l)
    GPU_PER_PROC=$((NUM_GPUS / NPROC))

    echo "Configuration:"
    echo "  Total GPUs: ${NUM_GPUS}"
    echo "  Parallel processes: ${NPROC}"
    echo "  GPUs per process: ${GPU_PER_PROC}"
    echo "  Expected speedup: ~${NPROC}x"
    echo ""

    # Use torchrun for multi-GPU parallel inference
    # Each process will automatically get GPU_PER_PROC GPUs via CUDA_VISIBLE_DEVICES
    torchrun --nproc_per_node=${NPROC} run.py \
      --data ${DATASET_NAME} \
      --model ${MODEL_NAME} \
      --mode all \
      --work-dir outputs_eval \
      --verbose
    
    EVAL_EXIT_CODE=$?
    
    echo ""
    echo "=========================================="
    if [ $EVAL_EXIT_CODE -eq 0 ]; then
        echo "‚úÖ Evaluation completed successfully!"
        echo "=========================================="
        echo ""
        echo "Results location:"
        echo "  outputs_eval/${MODEL_NAME}/"
        echo ""
        echo "Visualization outputs:"
        echo "  ${THINKMORPH_SAVE_DIR}/"
    else
        echo "‚ùå Evaluation failed with exit code: ${EVAL_EXIT_CODE}"
        echo "=========================================="
        exit ${EVAL_EXIT_CODE}
    fi
    
  result:
    path: /exp_outputs
  datasets:
    - mountPath: /weka/oe-training-default
      source:
        weka: oe-training-default
  resources:
    gpuCount: 4
    sharedMemory: 200GiB
  context:
    priority: high
    preemptible: true
  constraints:
    cluster: [ai2/jupiter, ai2/ceres, ai2/saturn]
  hostNetworking: true
  leaderSelection: true

